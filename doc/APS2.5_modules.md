# Update

## 180707
* [D] [Range](#Range)
* [D] [Condition](#Condition)
* [D] [MappingDataSPy3](#Mapping)
* [A] [MappingDataSPy3](#Mapping)
* [D] [CloseSessionDPy3](#CloseSessionD)
* [D] [CloseSparkSessionUnivDPy3](#CloseSpark)
* [D] [DecisionTreeClasDPy3](#DTtrainSpark)
* [D] [DecisionTreeEvalDPy3](#DTevalSpark)
* [D] [FeatureSelectorDataDPy3](#FSSpark)
* [D] [FeatureTransformerFeatDPy3](#FTSpark)
* [D] [GetLogsDPy3](#GetLogs)
* [D] [OpenSessionDPy3](#OpenSession)
* [D] [PySparkShellDPy3](#ShellD)
* [D] [StartSparkSessionUnivDPy3](#GetSpark)
* [D] [WordCutDPy3](#WordCutD)
* [A] [PivotingDPy3](#Pivot_pyspark)
* [A] [GroupbySumDPy3](#GroupbySum_pyspark)
* [A] [CorrelationDPy3](#Correlate_pyspark)
* [A] [FeatureInfoDPy3](#FeatInfo_pyspark)
* [A] [ChiSquareTestDPy3](#ChiSquare_pyspark)
* [A] [VectorSlicerDPy3](#VecSlice_pyspark)
* [A] [VectorIndexerDPy3](#VecIdx_pyspark)
* [A] [VectorAssemblerDPy3](#VecAssemble_pyspark)
* [A] [StringIndexerDPy3](#StrIdx_pyspark)
* [A] [StandardScalerDPy3](#StdScale_pyspark)
* [A] [StackRowsDPy3](#StackRow_pyspark)
* [A] [QuantileDiscretizerDPy3](#QuantileDiscret_pyspark)
* [A] [PolynomialExpansionDPy3](#PolyExpansion_pyspark)
* [A] [PCADPy3](#PCA_pyspark)
* [A] [OneHotEncoderDPy3](#OneHot_pyspark)
* [A] [NormalizerDPy3](#Normlize_pyspark)
* [A] [MinMaxScalerDPy3](#MinMax_pyspark)
* [A] [MergeColsDPy3](#MergeCols_pyspark)
* [A] [MaxAbsScalerDPy3](#MaxAbsScl_pyspark)
* [A] [IndexToStringDPy3](#Idx2Str_pyspark)
* [A] [ImputerDPy3](#Impute_pyspark)
* [A] [FeatureHasherDPy3](#FeatHash_pyspark)
* [A] [DropNADPy3](#DropNan_pyspark)
* [A] [ChiSqSelectorDPy3](#ChiSelect_pyspark)
* [A] [FillNADPy3](#FillNan_pyspark)
* [A] [SplitDPy3](#Split_pyspark)
* [A] [DecisionTreeClassifierDPy3](#DTClas_pyspark)
* [A] [MultilayerPerceptronClassifierDPy2](#MLPClas_pyspark)
* [A] [NaiveBayesDPy2](#NB_pyspark)
* [A] [RandomForestClassifierDPy2](#RFClas_pyspark)
* [A] [GBTClassifierDPy2](#GBTClas_pyspark)
* [A] [LogisticRegressionDPy3](#LR3_pyspark)
* [A] [LogisticRegressionDPy2](#LR2_pyspark)
* [A] [DecisionTreeRegressorDPy3](#DTReg_pyspark)
* [A] [RandomForestRegressorDPy3](#RFReg_pyspark)
* [A] [LinearRegressionDPy3](#LR_pyspark)
* [A] [GeneralizedLinearRegressionDPy2](#GLR_pyspark)
* [A] [GBTRegressorDPy2](#GBTReg_pyspark)
* [A] [BisectingKMeansDPy3](#BisectingKMeans_pyspark)
* [A] [GaussianMixtureDPy3_copy](#GM_pyspark)
* [A] [LDADPy3_copy](#LDA_pyspark)
* [A] [KMeansDPy3](#Kmeans_pyspark)
* [A] [ParamSearchTVSPy3](#ParamSearchTV_pyspark)
* [A] [ParamSearchCVDPy3](#ParamSearchCV_pyspark)
* [A] [ModelPredictDPy3](#ModelPred_pyspark)
* [A] [ModelEvaluateDPy3](#ModelEval_pyspark)
* [A] [PipelineFitDPy3](#PipelineFit_pyspark)
* [A] [spark2pmmlDPy](#spark2pmml_pyspark)
* [A] [RegexTokenizerDPy3](#RegexToken_pyspark)
* [A] [TokenizerDPy3](#Token_pyspark)
* [A] [NGramDPy3](#NGram_pyspark)
* [A] [StopWordsRemoverDPy3](#StopWordsRemove_pyspark)
* [A] [IDFDPy3_copy](#IDF_pyspark)
* [A] [HashingTFDPy3](#HashTF_pyspark)
* [A] [Word2VecDPy3](#Word2Vec_pyspark)
* [A] [CountVectorDPy3](#CountVec_pyspark)
* [A] [keras_test](#keras_test)
* [A] [tensorflow_test](#tensorflow_test)
* [A] [pytorch_test](#pytorch_test)
* [A] [mxnet_test](#mxnet_test)
* [A] [imageclassifier_mxnet](#imageclassifier_mxnet)
* [A] [DataLoadMnistKeras](#DataLoadMnistKeras)
* [A] [DataPretreatmentMnistKeras](#DataPretreatmentMnistKeras)
* [A] [ModelMnistKeras](#ModelMnistKeras)
* [A] [ModelStructShowKeras](#ModelStructShowKeras)
* [A] [ModelTrainKeras](#ModelTrainKeras)
* [A] [ModelEvaluateKeras](#ModelEvaluateKeras)
* [A] [NormalVisuSR_all](#vis_R)
* [A] [NormalVisuSR_read_data](#load_R)
* [A] [plotly_ggplot2_pie_2dim](#plt_R)

## 180629
* [A] [ML_localfile2csv](#ML_local)
* [A] [ML_PreHandle](#ML_prehandle)
* [A] [ML_SampleData](#ML_sample)
* [A] [ML_CategoryFeatureHandle](#ML_category)
* [A] [ML_NumFeatureHandle](#ML_num)
* [A] [ML_SplitSet](#ML_split)
* [A] [ML_HyperparamsCV](#ML_HyperparamsCV)
* [A] [ML_MultiClassModelEval](#ML_MultiClassModelEval)
* [A] [ML_RegModelEval](#ML_RegModelEval)
* [A] [ML_BinaryClassModelEval](#ML_BinaryClass)
* [A] [ImbalanceDataSPy3](#Imbalance)
* [A] [FeatureInfoDPy3](#spark_info)
* [A] [ColsSelectDataDPy3](#spark_cols)
* [A] [MissingHandlingDPy3](#spark_missing)
* [A] [FeatureTransformerDPy3](#spark_transform)
* [A] [FeatureSelectionFeatDPy3](#spark_selection)
* [A] [LogisticRegressionDPy3](#spark_logistic)
* [M] [DateParsingDataSPy3](#DateParse)
* [M] [AssembleBaseLeanersSPy3](#ABLeaner)
* [M] [PKLtoSotrage](#PKLtoS)
* [A] DefineLabelSPy3_churn
* [A] DimentionSelectionSPy3_churn
* [A] FeatureEngineeringSPy3_churn
* [A] ML_NumFeatureHandle_churn
* [A] ML_BinaryClassModelEval_churn
* [A] FilterFastSPy3_churn
* [A] FeatureEngineeringSPy3_predict_churn
* [A] DimentionSelectionSPy3_predict_churn
* [A] DimentionSelectionDPy3_churn
* [A] [Range](#Range)
* [A] [Condition](#Condition)


## 180622
* [A] [MergeDataSPy3](#MergeData)
* [A] [DropDuplicatesDataSPy3](#DropDup)
* [A] [MappingDataSPy3](#Mapping)
* [A] [DateParsingDataSPy3](#DateParse)
* [A] [DecisionTreeClasSPy3](#DecisionClas)
* [A] [DecisionTreeRegSPy3](#DecisionReg)
* [M] [DataInfoUnivSPy3](#DInfoU)
* [M] [ReplaceDataSPy3](#ReplaceD)
* [M] [FormShowUnivSPy3](#FShowU)
* [M] [FormShowCSVUnivSPy3](#FShowCSVU)


## 180615
* [M] [LogisticPredSPy3](#LogiPred)
* [M] [ClasEvalNewSPy3](#CEval)
* [M] [LogisticRegr_WOE_ClasSPy3](#Logistic_WOE)
* [A] [QuantileTrasformerDataSPy3](#Quantile)
* [A] [LabelEncoderDataSPy3](#LabelEncoder)
* [A] [OneHotEncoderDataSPy3](#OneHot)


## 180601
* [D] [ClasEvalSPy3](#CEval)
* [A] [ClasEvalNewSPy3](#CEval)
* [A] [PKLtoSotrage](#PKLtoS)
* [A] [PKLfromSotrage](#PKLfromS)

## 180525

* [M] [ReplaceDataSPy3](#ReplaceD)
* [M] [ChangeTypeDataSPy3](#CTypeD)
* [M] [PmmlClasSPy3](#PmmlC)
* [M] [SQLUnivSPy3](#SQL)
* [M] [ConcatDataSPy3](#Concat)
* [M] [Date2DaysDataSPy3](#Data2Days)
* [M] [ColsSelectDataSPy3](#CSelect2D) 
* [M] [DataInfoUnivSPy3](#DInfoU)
* [A] [OpenSessionDPy3](#OpenSession)
* [A] [PySparkShellDPy3](#ShellD)
* [A] [GetLogsDPy3](#GetLogs)
* [A] [WordCutDPy3](#WordCutD)
* [A] [FastTextSPy3](#FastText)
* [A] [CloseSessionDPy3](#CloseSessionD)
* [A] [JDBCdownloaderUnivSPy3](#JDBC)
* [M] [ReplaceDataSPy3](#ReplaceD)
* [M] [MapLambdaDataSPy3](#MapLambda)
* [D] [DataTypes](#DataT)
* [D] [SplitXY](#split)
* [D] [MissingCheck](#MCheck)
* [D] [ValueCounts](#VC)
* [D] [VariablesSelection](#VS)
* [D] [BucketLowFrequency](#BLF) 
* [A] [ChiMergeDataSPy3](#ChiMerge)
* [A] [WOE_IV_DataSPy3](#WOEIV)
* [D] [StandardScaler](#sc)
* [D] [StandardScalerDataSPy3](#sc)
* [D] [FunctionTransformer](#ft)
* [D] [MinmaxScalerDataSPy3](#MM)
* [D] [PolyNomialFeatures](#poly)
* [D] [Sample](#sample)
* [D] [SampleDataSPy3](#sample)
* [D] [DataPreprocessing](#DP)
* [D] [AsType](#AsT)
* [D] [ClassMapping](#classmap)
* [D] [QuantileTransformer](#QTrans)
* [D] [Box](#box)
* [D] [Imbalance](#imba)
* [M] [SplitFeatSPy3](#SplitF)
* [D] [SelectFromModel](#sfm)
* [D] [PearsonCorrelation](#pearson)
* [D] [MINE](#mine)
* [D] [chi2](#chi) 
* [D] [Union](#union)
* [A] [LinearSVCSPy3](#LinearSVC)
* [M] [LogisticRegrSPy3](#LogisticR)
* [A] [LogisticRegr_WOE_ClasSPy3](#Logistic_WOE)
* [M] [RandomforestClasSPy3](#Rforest)
* [M] [StackingClasSPy3](#StackingC)
* [M] [XGboostClasSPy3](#XGboostC)
* [M] [DecisionTreeClasDPy3](#DTtrainSpark)
* [M] [DecisionTreeEvalDPy3](#DTevalSpark)
* [M] [AdaboostRegrSPy3](#AboostR)
* [M] [BaggingRegrSPy3](#BaggR)
* [M] [ExtratreesRegrSPy3](#ExtratreeR)
* [M] [GradientboostingRegrSPy3](#GboostingR)
* [M] [RandomforestRegrSPy3](#RforestR)
* [M] [ClasEvalSPy3](#CEval)
* [D] [ConfusionMatrix](#cnf)
* [D] [Prediction](#pred)
* [M] [RandomizedSearchSPy3](#RSearch)
* [M] [PlotLearningCurveSPy3](#PLCurve)
* [M] [PlotLearningCurveSPy3_BestModel](#PLCBest)
* [A] [LogisticPredSPy3](#LogiPred)
* [A] [TPOTSPy3](#TPOT)
* [M] [StartSparkSessionUnivDPy3](#GetSpark)
* [A] [zixun_word2vec_copy](#zixun)
* [A] [jiebatest](#jiebatest)
* [A] [word2cut](#word2cut)
* [A] [word2vec](#word2vec) 
* [A] [wordfitting](#wordfitting)

## 180518

* [A] [ConcatDataSPy3](#Concat)
* [A] [CSV2PKLUnivSPy3](#CSV2PKL)
* [A] [Date2DaysDataSPy3](#Data2Days)
* [A] [HDFSdownloaderUnivSPy3](#HDFS)
* [A] [HiveDownloaderUnivSPy3](#Hive)
* [A] [MapLambdaDataSPy3](#MapLambda)
* [A] [PKL2CSVUnivSPy3](#PKL2CSV)
* [A] [ReplaceDataSPy3](#ReplaceD)
* [A] [SQLUnivSPy3](#SQL)

## 180504 

* [A] [AdaboostClasSPy3](#AboostC)
* [A] [AdaboostRegrSPy3](#AboostR)
* [A] [AffinityPropagationClusSPy3](#AffinityProp)
* [A] [AgglomerativeClusSPy3](#Agglomerative)
* [A] [AssembleBaseLeanersSPy3](#ABLeaner)
* [A] [BaggingClasSPy3](#BaggC)
* [A] [BaggingRegrSPy3](#BaggR)
* [A] [BirchClusSPy3](#BirchC)
* [A] [ChangeTypeDataSPy3](#CTypeD)
* [A] [ClasEvalSPy3](#CEval)
* [A] [ClasPredictSPy3](#CPredict)
* [A] [ClasRocEvalSPy3a](#CREval)
* [A] [ColsDropDataSPy3](#CDropD)
* [A] [ColsSelectCSVSPy3](#CSelectCSV) 
* [A] [ColsSelectDataSPy3](#CSelect2D)
* [A] [ColsSelect2DataSPy3](#CSelect2Data)
* [A] [CorrXXFeatSpy3](#CXXF) 
* [A] [CorrXYFeatSPy3](#CXYF)
* [A] [DataDownloaderUnivSPy3](#DDownU)
* [A] [DataInfoUnivSPy3](#DInfoU)
* [A] [DBSCANClusSPy3](#DBSCAN)
* [A] [DummyFitDataSPy3](#DFitD)
* [A] [DummyTransformDataSPy3](#DTransformD)
* [A] [ExtratreesClasSPy3](#ExtratreeC)
* [A] [ExtratreesRegrSPy3](#ExtratreeR)
* [A] [FillNADataSPy3](#FNAD)
* [A] [FormShowUnivSPy3](#FShowU)
* [A] [FormShowCSVUnivSPy3](#FShowCSVU)
* [A] [GradientboostingClasSPy3](#Gboosting)
* [A] [GradientboostingRegrSPy3](#GboostingR)
* [A] [KMeansClusSPy3](#KMeansC)
* [A] [KMeans2ClusSPy3](#KMeans2C)
* [A] [KMeansVisuSPy3](#KMeansV)
* [A] [LogisticRegrSPy3](#LogisticR)
* [A] [MinMaxScalerFitDataSPy3](#MMSFitD)
* [A] [MinMaxScalerTransformDataSPy3](#MMSTransformD)
* [M] [MissingDropDataSPy3](#MDropD)
* [M] [MissingFillDataSPy3](#MFillD)
* [A] [MissingImputeDataSPy3](#MImputeD)
* [A] [PlotLearningCurveSPy3](#PLCurve)
* [A] [PlotLearningCurveSPy3_BestModel](#PLCBest)
* [A] [PmmlClasSPy3](#PmmlC)
* [A] [RandomforestClasSPy3](#Rforest)
* [A] [RandomforestRegrSPy3](#RforestR)
* [A] [RandomizedSearchSPy3](#RSearch)
* [A] [RegrEvalSPy3](#REvalS)
* [A] [ReportPDFClasEvalSPy3](#RPDFCE)
* [A] [RFEFeatSPy3](#RFEF)
* [A] [SpectralClusSPy3](#SpectralC)
* [A] [SplitFeatSPy3](#SplitF)
* [A] [StackingClasSPy3](#StackingC)
* [A] [StackingPredictSPy3](#StackingPredict)
* [A] [VarianceThresholdFitFeatSPy3](#VTFitF)
* [A] [VarianceThresholdTransformFeatSPy3](#VTTransformF)
* [A] [XGboostClasSPy3](#XGboostC)
* [D] [RFE](#rfe)
* [D] [AdaBoost](#Ada)
* [D] [xgboost](#xg)
* [D] [Stacking](#stack)


## 180411

* [A] [StartSparkSessionUnivDPy3](#GetSpark)
* [A] [FeatureSelectorDataDPy3](#FSSpark)
* [A] [FeatureTransformerFeatDPy3](#FTSpark)
* [A] [DecisionTreeClasDPy3](#DTtrainSpark)
* [A] [DecisionTreeEvalDPy3](#DTevalSpark)
* [A] [CloseSparkSessionUnivDPy3](#CloseSpark)

## 180410

* [A] [MissingDropDataSPy3](#MDropD)
* [A] [MissingFillDataSPy3](#MFillD)
* [A] [SampleDataSPy3](#sample)
* [A] [BoxDataSPy3](#box)
* [A] [MinmaxScalerDataSPy3](#MM)
* [A] [StandardScalerDataSPy3](#sc)
* [A] [Chi2CorrFeatSPy3](#chi)

## 180223

* [A] [VariablesSelection](#VS)
* [A] [DataPreprocessing](#DP)
* [A] [DataTypes](#DataT)
* [A] [MissingCheck](#MCheck)
* [A] [AsType](#AsT)
* [A] [ClassMapping](#classmap)
* [A] [QuantileTransformer](#QTrans)
* [A] [Box](#box)
* [A] [SplitXY](#split)
* [A] [chi2](#chi) 
* [A] [PearsonCorrelation](#pearson)
* [A] [SelectFromModel](#sfm)
* [A] [Union](#union)
* [A] [Imbalance](#imba)
* [A] [TrainTestSplit](#split)
* [A] [Stacking](#stack)
* [A] [ConfusionMatrix](#cnf)
* [M] [AdaBoost](#Ada)
* [A] [Prediction](#pred)

## 180209

* [A] [Sample](#sample)
* [A] [ChurnLabel](#CL)
* [A] [ValueCounts](#VC)
* [A] [BucketLowFrequency](#BLF) 

## 180205

* [A] [HashingEncoder](#he)
* [A] [AdaBoost](#Ada)
* [A] [xgboost](#xg)
* [A] [StandardScaler](#sc)
* [A] [SelectFromModel](#sfm)
* [A] [PearsonCorrelation](#pearson)
* [A] [RFE](#rfe)
* [A] [MINE](#mine)
* [A] [chi2](#chi) 
* [A] [FunctionTransformer](#ft)
* [A] [PolyNomialFeatures](#poly)

# Index
# Python 单机版

## utils 通用工具
* [CSV2PKLUnivSPy3](#CSV2PKL)
* [DataDownloaderUnivSPy3](#DDownU)
* [DataInfoUnivSPy3](#DInfoU)
* [HDFSdownloaderUnivSPy3](#HDFS)
* [HiveDownloaderUnivSPy3](#)
* [JDBCdownloaderUnivSPy3](#JDBC)
* [PKL2CSVUnivSPy3](#PKL2CSV)
* [PKLtoSotrage](#PKLtoS)
* [PKLfromSotrage](#PKLfromS)

## SQL sql语言
* [SQLUnivSPy3](#SQL)

## dataframe 数据操作
* [ChangeTypeDataSPy3](#CTypeD)
* [ColsDropDataSPy3](#CDropD)
* [ColsSelectCSVSPy3](#CSelectCSV) 
* [ColsSelectDataSPy3](#CSelect2D) 
* [ColsSelect2DataSPy3](#CSelect2Data) 
* [ConcatDataSPy3](#Concat)
* [Date2DaysDataSPy3](#Data2Days)
* [DateParsingDataSPy3](#DateParse)
* [DropDuplicatesDataSPy3](#DropDup)
* [FillNADataSPy3](#FNAD)
* [MapLambdaDataSPy3](#MapLambda)
* [MappingDataSPy3](#Mapping)
* [MergeDataSPy3](#MergeData)
* [ReplaceDataSPy3](#ReplaceD)
* [SplitFeatSPy3](#SplitF)

## preprocessing 预处理
* [ChiMergeDataSPy3](#ChiMerge)
* [DummyFitDataSPy3](#DFitD)
* [DummyTransformDataSPy3](#DTransformD)
* [ImbalanceDataSPy3](#Imbalance)
* [LabelEncoderDataSPy3](#LabelEncoder)
* [MinMaxScalerFitDataSPy3](#MMSFitD)
* [MinMaxScalerTransformDataSPy3](#MMSTransformD)
* [MissingDropDataSPy3](#MDropD)
* [MissingFillDataSPy3](#MFillD)
* [MissingImputeDataSPy3](#MImputeD)
* [OneHotEncoderDataSPy3](#OneHot)
* [QuantileTrasformerDataSPy3](#Quantile)
* [WOE_IV_DataSPy3](#WOEIV)

## feature_selection 特征选择
* [CorrXXFeatSpy3](#CXXF)
* [CorrXYFeatSPy3](#CXYF)
* [RFEFeatSPy3](#RFEF)
* [VarianceThresholdFitFeatSPy3](#VTFitF)
* [VarianceThresholdTransformFeatSPy3](#VTTransformF)

## classifier 分类模型

* [AdaboostClasSPy3](#AboostC)
* [AssembleBaseLeanersSPy3](#ABLeaner)
* [BaggingClasSPy3](#BaggC)
* [DecisionTreeClasSPy3](#DecisionClas)
* [ExtratreesClasSPy3](#ExtratreeC)
* [GradientboostingClasSPy3](#Gboosting)
* [LinearSVCSPy3](#LinearSVC)
* [LogisticRegrSPy3](#LogisticR)
* [LogisticRegr_WOE_ClasSPy3](#Logistic_WOE)
* [RandomforestClasSPy3](#Rforest)
* [StackingClasSPy3](#StackingC)
* [XGboostClasSPy3](#XGboostC)

## cluster 聚类
* [AffinityPropagationClusSPy3](#AffinityProp)
* [AgglomerativeClusSPy3](#Agglomerative)
* [BirchClusSPy3](#BirchC)
* [DBSCANClusSPy3](#DBSCAN)
* [KMeansClusSPy3](#KMeansC)
* [KMeans2ClusSPy3](#KMeans2C)
* [KMeansVisuSPy3](#KMeansV)
* [SpectralClusSPy3](#SpectralC)

## regressor 回归模型
* [AdaboostRegrSPy3](#AboostR)
* [BaggingRegrSPy3](#BaggR)
* [DecisionTreeRegSPy3](#DecisionReg)
* [ExtratreesRegrSPy3](#ExtratreeR)
* [GradientboostingRegrSPy3](#GboostingR)
* [RandomforestRegrSPy3](#RforestR)

## natural_language_processing 自然语言处理
* [FastTextSPy3](#FastText)
* [jiebatest](#jiebatest)
* [word2cut](#word2cut)
* [word2vec](#word2vec) 
* [WordCutDPy3](#WordCutDPy3)
* [wordfitting](#wordfitting)
* [zixun_word2vec_copy](#zixun)

## hyperparameters_tuning 模型调参
* [RandomizedSearchSPy3](#RSearch)

## prediction 模型预测
* [ClasPredictSPy3](#CPredict)
* [LogisticPredSPy3](#LogiPred)
* [StackingPredictSPy3](#StackingPredict)

## metrics 评估指标
* [ClasEvalNewSPy3](#CEval)
* [ClasRocEvalSPy3a](#CREval)
* [PlotLearningCurveSPy3](#PLCurve)
* [PlotLearningCurveSPy3_BestModel](#PLCBest)
* [RegrEvalSPy3](#REvalS)
* [ReportPDFClasEvalSPy3](#RPDFCE)

## model_release 模型发布
* [PmmlClasSPy3](#PmmlC)

## visualization 可视化
* [ClasEvalSPy3](#CEval)
* [ClasRocEvalSPy3a](#CREval)
* [FormShowUnivSPy3](#FShowU)
* [FormShowCSVUnivSPy3](#FShowCSVU)
* [KMeansVisuSPy3](#KMeansV)
* [PlotLearningCurveSPy3](#PLCurve)
* [PlotLearningCurveSPy3_BestModel](#PLCBest)
* [ReportPDFClasEvalSPy3](#RPDFCE)
* [WOE_IV_DataSPy3](#WOEIV)

## AutoML 自动建模
* [ML_localfile2csv](#ML_local)
* [ML_PreHandle](#ML_prehandle)
* [ML_SampleData](#ML_sample)
* [ML_CategoryFeatureHandle](#ML_category)
* [ML_NumFeatureHandle](#ML_num)
* [ML_SplitSet](#ML_split)
* [ML_HyperparamsCV](#ML_HyperparamsCV)
* [ML_MultiClassModelEval](#ML_MultiClassModelEval)
* [ML_RegModelEval](#ML_RegModelEval)
* [ML_BinaryClassModelEval](#ML_BinaryClass)
* [TPOTSPy3](#TPOT)

## deep_learning 深度学习
* [keras_test](#keras_test)
* [tensorflow_test](#tensorflow_test)
* [pytorch_test](#pytorch_test)
* [mxnet_test](#mxnet_test)
* [imageclassifier_mxnet](#imageclassifier_mxnet)
* [DataLoadMnistKeras](#DataLoadMnistKeras)
* [DataPretreatmentMnistKeras](#DataPretreatmentMnistKeras)
* [ModelMnistKeras](#ModelMnistKeras)
* [ModelStructShowKeras](#ModelStructShowKeras)
* [ModelTrainKeras](#ModelTrainKeras)
* [ModelEvaluateKeras](#ModelEvaluateKeras)


## R R语言
* [NormalVisuSR_all](#vis_R)
* [NormalVisuSR_read_data](#load_R)
* [plotly_ggplot2_pie_2dim](#plt_R)


# PySpark 分布式

## data_exploration_pyspark 数据探索_分布式
* [PivotingDPy3](#Pivot_pyspark)
* [GroupbySumDPy3](#GroupbySum_pyspark)
* [CorrelationDPy3](#Correlate_pyspark)
* [FeatureInfoDPy3](#FeatInfo_pyspark)
* [ChiSquareTestDPy3](#ChiSquare_pyspark)

## preprocessing_pyspark 预处理_分布式

* [VectorSlicerDPy3](#VecSlice_pyspark)
* [VectorIndexerDPy3](#VecIdx_pyspark)
* [VectorAssemblerDPy3](#VecAssemble_pyspark)
* [StringIndexerDPy3](#StrIdx_pyspark)
* [StandardScalerDPy3](#StdScale_pyspark)
* [StackRowsDPy3](#StackRow_pyspark)
* [QuantileDiscretizerDPy3](#QuantileDiscret_pyspark)
* [PolynomialExpansionDPy3](#PolyExpansion_pyspark)
* [PCADPy3](#PCA_pyspark)
* [OneHotEncoderDPy3](#OneHot_pyspark)
* [NormalizerDPy3](#Normlize_pyspark)
* [MinMaxScalerDPy3](#MinMax_pyspark)
* [MergeColsDPy3](#MergeCols_pyspark)
* [MaxAbsScalerDPy3](#MaxAbsScl_pyspark)
* [IndexToStringDPy3](#Idx2Str_pyspark)
* [ImputerDPy3](#Impute_pyspark)
* [FeatureHasherDPy3](#FeatHash_pyspark)
* [DropNADPy3](#DropNan_pyspark)
* [ChiSqSelectorDPy3](#ChiSelect_pyspark)
* [FillNADPy3](#FillNan_pyspark)
* [SplitDPy3](#Split_pyspark)

## classifier_pyspark 分类模型_分布式

* [DecisionTreeClassifierDPy3](#DTClas_pyspark)
* [MultilayerPerceptronClassifierDPy2](#MLPClas_pyspark)
* [NaiveBayesDPy2](#NB_pyspark)
* [RandomForestClassifierDPy2](#RFClas_pyspark)
* [GBTClassifierDPy2](#GBTClas_pyspark)
* [LogisticRegressionDPy3](#LR3_pyspark)
* [LogisticRegressionDPy2](#LR2_pyspark)

## regressor_pyspark 回归模型_分布式

* [DecisionTreeRegressorDPy3](#DTReg_pyspark)
* [RandomForestRegressorDPy3](#RFReg_pyspark)
* [LinearRegressionDPy3](#LR_pyspark)
* [GeneralizedLinearRegressionDPy2](#GLR_pyspark)
* [GBTRegressorDPy2](#GBTReg_pyspark)

## cluster_pyspark 聚类模型_分布式
* [BisectingKMeansDPy3](#BisectingKMeans_pyspark)
* [GaussianMixtureDPy3_copy](#GM_pyspark)
* [LDADPy3_copy](#LDA_pyspark)
* [KMeansDPy3](#Kmeans_pyspark)

## model_selection & hyperparameters_tuning_pyspark 模型选择以及调参_分布式
* [ParamSearchTVSPy3](#ParamSearchTV_pyspark)
* [ParamSearchCVDPy3](#ParamSearchCV_pyspark)

## prediction & metrics_pyspark 模型预测以及评估_分布式
* [ModelPredictDPy3](#ModelPred_pyspark)
* [ModelEvaluateDPy3](#ModelEval_pyspark)

## model_release_pyspark 模型发布_分布式
* [PipelineFitDPy3](#PipelineFit_pyspark)
* [spark2pmmlDPy](#spark2pmml_pyspark)

## natural_language_processing_pyspark 自然语言处理_分布式
* [RegexTokenizerDPy3](#RegexToken_pyspark)
* [TokenizerDPy3](#Token_pyspark)
* [NGramDPy3](#NGram_pyspark)
* [StopWordsRemoverDPy3](#StopWordsRemove_pyspark)
* [IDFDPy3_copy](#IDF_pyspark)
* [HashingTFDPy3](#HashTF_pyspark)
* [Word2VecDPy3](#Word2Vec_pyspark)
* [CountVectorDPy3](#CountVec_pyspark)

# Module

## <a id="AboostC">AdaboostClasSPy3</a>
一种对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来构成一个更强的最终分类器(强分类器)的迭代算法。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。

#### Tag:

* classifier

#### Param:

* n_estimators (int): 评估器数量
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="AboostR">AdaboostRegrSPy3</a>
一种对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来构成一个更强的最终分类器(强分类器)的迭代算法。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。

#### Tag:

* regressor

#### Param:

* n_estimators (int): 评估器数量
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_import_feat (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="AffinityProp">AffinityPropagationClusSPy3</a>
Affinity Propagation Clustering（吸引力传播聚类，简称AP算法）是2007在Science上发表的一篇single-exemplar-based的聚类方面的文章。特别适合高维、多类数据快速聚类，相比传统的聚类算法，从聚类性能和效率方面都有大幅度的提升, AP算法不需要事先给定聚类中心个数，算法在迭代过程中展示数据集的内部结构，并确定合适的聚类个数，同时效率非常高

#### Tag:

* cluster

#### Param:

* damping (double): 阻尼系数（介于0.5和1之间）是相对于输入值（加权1-阻尼）的维持程度，避免数值振荡
* max_iter (int): 迭代次数
* verbose (int): 日志

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量
 
#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型
* o_cluster_centers_indices (csv): 聚类中心对应的索引


## <a id="Agglomerative">AgglomerativeClusSPy3</a>
是一种自底而上的层次聚类方法，它能够根据指定的相似度或距离定义计算出类之间的距离。它从每一个点开始作为一个类，然后迭代的融合最近的类。能创建一个树形层次结构的聚类模型。

#### Tag:

* cluster

#### Param:

* n_clusters (int): 指定聚类数
* affinity (string): 距离算法 (euclidean/l1/l2/manhattan/cosine/precomputed)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量
 
#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型


## <a id="ABLeaner">AssembleBaseLeanersSPy3</a>
将多个模型组成一个list形成Base Learner以便传入Stacking模型中

#### Tag:

* classifier

#### Param:

* None

#### Input:

* model1 (py3pkl): 模型1
* model2 (py3pkl): 模型2
* model3 (py3pkl): 模型3
* model4 (py3pkl): 模型4
* model5 (py3pkl): 模型5

#### Output:

* m_base_learners (py3pkl): 5个模型组成的Base Learner


## <a id="BaggC">BaggingClasSPy3</a>
bagging是一种用来提高学习算法准确度的方法，这种方法通过构造一个预测函数系列，然后以一定的方式将它们组合成一个预测函数。Bagging要求“不稳定”（不稳定是指数据集的小的变动能够使得分类结果的显著的变动）的分类方法。比如：决策树，神经网络算法。

#### Tag:

* classifier

#### Param:

* n_estimators (int): 评估器数量
* max_samples (double): 最大采样比率
* max_features (double): 最大特征比率

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="BaggR">BaggingRegrSPy3</a>
bagging是一种用来提高学习算法准确度的方法，这种方法通过构造一个预测函数系列，然后以一定的方式将它们组合成一个预测函数。Bagging要求“不稳定”（不稳定是指数据集的小的变动能够使得分类结果的显著的变动）的分类方法。比如：决策树，神经网络算法。

#### Tag:

* regressor

#### Param:

* n_estimators (int): 评估器数量
* max_samples (double): 最大采样比率

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="BirchC">BirchClusSPy3</a>
是一个综合的层次聚类算法。它用到了聚类特征(Clustering Feature, CF)和聚类特征树(CF Tree)两个概念，用于概括聚类描述,该算法能够用一 遍扫描有效地进行聚类，并能够有效地处理离群点。Birch算法是基于距离的层次聚类，综合了层次凝聚和迭代的重定位方法

#### Tag:

* cluster

#### Param:

* n_clusters (int): 指定聚类数
* threshold (double): 叶节点每个CF的最大样本半径阈值，它决定了每个CF里所有样本形成的超球体的半径阈值。

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量
 
#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型


## <a id="CTypeD">ChangeTypeDataSPy3</a>
转换指定列的数据类型

#### Tag:

* dataframe

#### Param:

* cols (string): 选择要转换类型的变量
* cols1 (string): 选择要剔除的变量，其余变量全部做类型转换；默认为#时，使用cols做转换。
* type (string): 转换后的类型(object, int64, float64) 

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl)： 指定列转换类型后的数据


## <a id="ChiMerge">ChiMergeDataSPy3</a>
卡方分箱法：自低向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起，直到满足确定的停止准则。
基本思想：对于精确的离散化，相对类频率在一个区间内应当完全一致。因此，如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。

#### Tag:

* preprocessing

#### Param:

* label (string)：定义标签变量


#### Input:

* d_data1 (py3pkl): 输入数据

#### Output:

* d_data1 (py3pkl): 分箱后数据
* all_var (py3pkl): 分箱的变量及种类数小于5未分箱的类别变量


## <a id="CEval">ClasEvalNewSPy3</a>
对二分类及多分类模型进行评估（包括AUC，Kappa，评估报告及混淆矩阵等）

#### Tag:

* metrics
* visualization

#### Param:

* None

#### Input:

* d_true (csv): 真实标签
* d_pred (csv): 预测的标签变量
* d_prob (csv): 预测概率

#### Output:

* o_metric (csv): 各评估指标（准确率、Kappa分数、F分数、ROC值等）
* o_classification_report (txt): 评估报告(F分数、精确率、召回率)
* o_confusion_matrix (jpg): 混淆矩阵图


## <a id="CPredict">ClasPredictSPy3</a>
通过已训练好的模型进行预测

#### Tag:

* prediction

#### Param:

* None

#### Input:

* d_feature (csv): 特征变量
* m_fitted_model (py3pkl): 算法训练好的模型

#### Output:

* d_predict (csv): 预测值及预测概率
* d_pred (csv): 预测值
* d_prob (csv): 预测概率


## <a id="CREval">ClasRocEvalSPy3a</a>
输出分类模型的ROC曲线

#### Tag:

* metrics
* visualization

#### Param:

* None

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量
* m_fitted_model (py3pkl): 训练好的模型

#### Output:

* o_roc_curve (jpg): ROC曲线图


## <a id="CDropD">ColsDropDataSPy3</a>
删除指定列

#### Tag:

* dataframe

#### Param:

* cols (string): 选择要删除的变量

#### Input:

* d_data (csv): 数据

#### Output:

* d_changed_data (py3pkl): 删除指定列后的数据


## <a id="CSelectCSV">ColsSelectCSVSPy3</a>
从dataframe选择需要的变量

#### Tag:

* dataframe

#### Param:

* cols (string): 选择需要的变量

#### Input:

* d_data (csv): 数据

#### Output:

* d_selected_data (csv): 变量选择后的dataframe


## <a id="CSelect2D">ColsSelectDataSPy3</a>
从dataframe选择需要的变量

#### Tag:

* dataframe

#### Param:

* cols (string): 选择需要的变量

#### Input:

* d_data (csv): 数据

#### Output:

* d_selected_data (py3pkl): 变量选择后的dataframe


## <a id="CSelect2Data">ColsSelect2DataSPy3</a>
从dataframe选择需要的变量

#### Tag:

* dataframe

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据
* selected_cols (py3pkl): 选择的变量

#### Output:

* d_selected_data (csv)： 选择变量后的dataframe


## <a id="Concat">ConcatDataSPy3</a>
合并两个数据集

#### Tag:

* dataframe

#### Param:

* None

#### Input:

* d_data1 (csv): 数据1
* d_data2 (csv): 数据2
 
#### Output:

* d_data (csv): 合并后的数据


## <a id="CXXF">CorrXXFeatSpy3</a>
计算特征变量和特征变量之间的(pearson/spearman/kendall)相关性，并通过设定的参数来消除强相关的特征变量

#### Tag:

* feature_selection

#### Param:

* corr_type (string): 计算相关性方法
* corr_threshold (double): 消除强相关变量的阈值

#### Input:

* d_feature (csv): 数据
* o_featrue_label_corr (csv): 与标签变量间的相关性分数 (chi2/互信息/F检验分数)

#### Output:

* d_feature_selected (csv): 相关性筛选后的数据
* o_corr_XX (html): 相关性矩阵
* o_corr_heatmap (jpg): 相关性热力图


## <a id="CXYF">CorrXYFeatSPy3</a>
计算特征变量和标签变量之间的相关性（卡方/互信息/F检验），并通过设定的参数来筛选相应的特征变量

#### Tag:

* feature_selection

#### Param:

* feature_percent (int): 保留变量个数百分比 (0-100)
* sample_rate (double): 抽样比例 (0-1)

#### Input:

* d_feature (py3pkl): 目标变量
* d_label (py3pkl): 标签变量

#### Output:

* d_feature_selected (csv): 相关性筛选后的数据
* o_featrue_label_corr (csv): 与标签变量间的相关性分数 (卡方/互信息/F检验分数)



## <a id="CSV2PKL">CSV2PKLUnivSPy3</a>
将csv转换为pkl格式

#### Tag:

* utils

#### Param:

* None

#### Input:

* d_data1 (csv): csv数据
 
#### Output:

* d_data2 (py3pkl): 返回pickle格式数据


## <a id="DDownU">DataDownloaderUnivSPy3</a>
解析数据模块表示的资源引用，然后导入工作流。

#### Tag:

* utils

#### Param:

* None

#### Input:

* data_source (datasource.file): 从数据模块获取到的路径

#### Output:

* data (any): 数据


## <a id="DInfoU">DataInfoUnivSPy3</a>
数据统计：数据类型与缺失值比例； 均值，标准差，最小值，25分位数，50分位数，75分位数，最大值

#### Tag:

* utils

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据

#### Output:

* o_data_type_null (html): 数据类型与缺失值统计
* o_data_describe (html): 均值，标准差，最小值，25分位数，50分位数，75分位数，最大值


## <a id="Data2Days">Date2DaysDataSPy3</a>
将日期转换为天数(可以是两个日期间距离的天数也可以是一个日期距今的天数)

#### Tag:

* dataframe

#### Param:

* var1 (string): 日期变量1
* var2 (string): 日期变量2
* whether_use_2_variables (int): 是否使用两个日期变量，如果否，则只使用var1，计算var1距当前时间天数
* new_var (string): 新生成变量的变量名

#### Input:

* d_data1 (py3pkl): 数据
 
#### Output:

* d_data2 (py3pkl): 日期处理后的数据


## <a id="DateParse">DateParsingDataSPy3</a>
解析日期型变量，生成四个新变量：月、日、一周的第几天、是否为周末

#### Tag:

* dataframe

#### Param:

* cols (string): 选择要构建衍生变量的日期
* format (string): 指定日期变量的格式	

#### Input:

* d_data1 (py3pkl): 输入数据

#### Output:

* d_changed_data (py3pkl): 衍生变量后数据


## <a id="DBSCAN">DBSCANClusSPy3</a>
是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。

#### Tag:

* cluster

#### Param:

* eps (double): 邻域中样本最大距离
* min_samples (int): 邻域内最小样本数

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量
 
#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型


## <a id="DecisionClas">DecisionTreeClasSPy3</a>
决策树(decision tree)是一种基本的分类与回归方法。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。
决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。

#### Tag:

* classifier

#### Param:

* min_impurity_split (double): 预剪枝时节点的不纯度(gini或者entropy)低于此阈值就不再分裂; 注：9999.0表示默认值None
* max_depth (int): 树的最大深度。默认值不限制，当不设置的时候，树不能无限生长，它会由其他参数来控制它的生长；9999.0表示默认值None
* min_samples_split (int): 节点继续往下分裂的最小样本数要求
* min_samples_leaf (int): 叶子节点的最小样本数要求
* max_features (double): 节点分裂时使用的最大特征数，数据集比较大的时候，对特征进行抽样，提高速度；注：注：9999.0表示默认值None
* criterion (string): 节点最佳分裂的判断标准，可选值'gini'和'entropy'，默认值'gini'

#### Input:

* d_feature_test (csv): 特征变量
* d_label_test (csv): 目标变量


#### Output:



## <a id="DecisionReg">DecisionTreeRegSPy3</a>
决策树(decision tree)是一种基本的分类与回归方法。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。
决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。

#### Tag:

* regressor

#### Param:

* min_impurity_split (double): 预剪枝时节点的不纯度(gini或者entropy)低于此阈值就不再分裂; 注：9999.0表示默认值None
* max_depth (int): 树的最大深度。默认值不限制，当不设置的时候，树不能无限生长，它会由其他参数来控制它的生长；9999.0表示默认值None
* min_samples_split (int): 节点继续往下分裂的最小样本数要求
* min_samples_leaf (int): 叶子节点的最小样本数要求
* max_features (double): 节点分裂时使用的最大特征数，数据集比较大的时候，对特征进行抽样，提高速度；注：9999.0表示默认值None
* criterion (string): 节点最佳分裂的判断标准，可选值'gini'和'entropy'，默认值'gini'

#### Input:

* d_feature_test (csv): 特征变量
* d_label_test (csv): 目标变量


#### Output:



## <a id="DropDup">DropDuplicatesDataSPy3</a>
根据指定变量去掉重复行

#### Tag:

* dataframe

#### Param:

* col (string): 指定变量
* method (string): 选择去重方法

#### Input:

* d_data1 (csv): 数据

#### Output:

* d_changed_data (csv): 去重后数据


## <a id="DFitD">DummyFitDataSPy3</a>
对数据做哑编码转化(训练数据时使用，针对类别型变量)

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_dummy_data (py3pkl): 哑编码后的数据
* cols (py3pkl): 哑编码后的所有变量


## <a id="DTransformD">DummyTransformDataSPy3</a>
对数据做哑编码转化(将数据转化为同训练数据相同的变量)

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据
* cols (py3pkl): 训练数据哑编码后的所有变量

#### Output:

* d_dummy_data (py3pkl): 哑编码后的数据


## <a id="ExtratreeC">ExtratreesClasSPy3</a>
这个类实现了一个元估计器，该估计器适合于数据集的各个子样本上的多个随机决策树（又名extra-trees），并使用平均值来提高预测准确度和控制过度拟合。

#### Tag:

* classifier

#### Param:

* n_estimators (int): 评估器数量
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="ExtratreeR">ExtratreesRegrSPy3</a>
这个类实现了一个元估计器，该估计器适合于数据集的各个子样本上的多个随机决策树（又名extra-trees），并使用平均值来提高预测准确度和控制过度拟合。

#### Tag:

* regressor

#### Param:

* n_estimators (int): 评估器数量
* criterion (string): 评估算法

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_importance_feat (csv): 特征重要性
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="FastText">FastTextSPy3</a>
FastText是Facebook开发的一款快速文本分类器，提供简单而高效的文本分类和表征学习的方法

#### Tag:

* natural_language_processing

#### Param:

* model (string): skipgram 或者 cbow
* lr (double): 学习速率
* dim (int): 词向量维度

 
#### Input:

* word (txt): 分词后的数据

#### Output:

* model (bin): 包含模型参数的二进制文件以及字典和所有超参数。
* vec (bin): 词向量, 一词一行


## <a id="FNAD">FillNADataSPy3</a>
将指定变量的缺失值全部填补为0

#### Tag:

* dataframe

#### Param:

* cols: string

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl)： 特定列缺失填补完的数据


## <a id="FShowU">FormShowUnivSPy3</a>
以HTML形式显示DataFrame

#### Tag:

* visualization

#### Param:

* None

#### Input:

* d_data (py3pkl): pickle形式的数据

#### Output:

* d_form (html): html形式的数据


## <a id="FShowCSVU">FormShowCSVUnivSPy3</a>
以HTML形式显示DataFrame

#### Tag:

* visualization

#### Param:

* None

#### Input:

* d_data (csv): csv形式的数据

#### Output:

* d_form (html): html形式的数据



## <a id="Gboosting">GradientboostingClasSPy3</a>
和Adaboost不同，Gradient Boosting 在迭代的时候选择梯度下降的方向来保证最后的结果最好。 损失函数用来描述模型的“靠谱”程度，假设模型没有过拟合，损失函数越大，模型的错误率越高。 如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度方向上下降。

#### Tag:

* classfier

#### Param:

* n_estimators (int): 评估器数量
* loss (string): 损失函数
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="GboostingR">GradientboostingRegrSPy3</a>
Gradient Boosting 在迭代的时候选择梯度下降的方向来保证最后的结果最好。 损失函数用来描述模型的“靠谱”程度，假设模型没有过拟合，损失函数越大，模型的错误率越高 如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度方向上下降。

#### Tag:

* regressor

#### Param:

* n_estimators (int): 评估器数量
* loss (string): 损失函数
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_importance_feat (csv): 特征重要性
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="HDFS">HDFSdownloaderUnivSPy3</a>
从HDFS路径获取数据

#### Tag:

* utils

#### Param:

* None

#### Input:

* o_path (any): hdfs路径
 
#### Output:

* d_data (csv): 获取到的数据


## <a id="Hive">HiveDownloaderUnivSPy3</a>
从Hive对应数据库获取数据

#### Tag:

* utils

#### Param:

* script (string): sql语言，例如：select * from iris limit 5

#### Input:

* o_path (any): hive路径
 
#### Output:

* d_data (csv): 获取到的数据


## <a id="Imbalance">ImbalanceDataSPy3</a>
对数据进行过采样处理

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* X (csv): X
* Y (csv): Y

#### Output:

* x_resampled (csv): 过采样后X
* y_resampled (csv): 过采样后Y


## <a id="JDBC">JDBCdownloaderUnivSPy3</a>
通过JDBC方式，访问数据库。

#### Tag:

* utils

#### Param:

* jdbc_url (string)：数据库JDBC连接串
* driver_jar (string)：数据库驱动路径
* query (int)：SQL语句
* driver_name (string)：驱动类名
* delimiter (string)：数据的分隔符

#### Input:

* None

#### Output:

* output_file (any): 输出数据


## <a id="jiebatest">jiebatest</a>
load进gensim生成的词向量模型，打印词向量

#### Tag:

* natural_language_processing

#### Param:

* None

#### Input:

* model (model): gensim训练的词向量模型

#### Output:

* None


## <a id="KMeansC">KMeansClusSPy3</a>
K-means算法是硬聚类算法，是典型的基于原型的目标函数聚类方法的代表，它是数据点到原型的某种距离作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。K-means算法以欧式距离作为相似度测度，它是求对应某一初始聚类中心向量V最优分类，使得评价指标J最小。算法采用误差平方和准则函数作为聚类准则函数。

#### Tag:

* cluster

#### Param:

* n_clusters (int): 指定聚类个数
* max_iter (int): 迭代次数

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量
 
#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型


## <a id="KMeans2C">KMeans2ClusSPy3</a>
K-means算法是硬聚类算法，是典型的基于原型的目标函数聚类方法的代表，它是数据点到原型的某种距离作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。K-means算法以欧式距离作为相似度测度，它是求对应某一初始聚类中心向量V最优分类，使得评价指标J最小。算法采用误差平方和准则函数作为聚类准则函数。

#### Tag:

* cluster

#### Param:

* k (int): 指定聚类个数
* max_iter (int): 迭代次数

#### Input:

* d_feature (csv): 特征变量
 
#### Output:

* o_centers (py3pkl): 聚类中心
* o_labels (py3pkl): 预测聚类
* o_metrics (py3pkl): 轮廓系数和intertia
* o_distortions (py3pkl): 畸变函数


## <a id="KMeansV">KMeansVisuSPy3</a>
聚类中心雷达图和肘部图

#### Tag:

* cluster
* visualization

#### Param:

* None

#### Input:

* o_centers (py3pkl): 聚类中心
* o_distortions (py3pkl): 畸变函数
 
#### Output:

* distortion_plot: 肘部图 
* centers_plot (py3pkl): 聚类中心雷达图


## <a id="LabelEncoder">LabelEncoderDataSPy3</a>
对类别型文本变量进行编码

#### Tag:

* preprocessing

#### Param:

* col (string): 手动选择要进行数值编码的列
* signal (int): 选择是否自动搜索变量类型，然后进行筛选。 1为选择自动搜索


#### Input:

* d_data (py3pkl): 输入数据

#### Output:

* d_changed_data (py3pkl): 数值编码转换后数据


## <a id="LinearSVC">LinearSVCSPy3</a>
支持向量机的优势在于:
1. 在高维空间中非常高效
2. 即使在数据维度比样本数量大的情况下仍然有效
3. 在决策函数（称为支持向量）中使用训练集的子集,因此它也是高效利用内存的
4. 通用性: 不同的核函数与特定的决策函数一一对应

支持向量机的缺点包括:
如果特征数量比样本数量大得多,在选择核函数时要避免过拟合,而且正则化项是非常重要的

#### Tag:

* classifier

#### Param:

* kernel (string)：核函数，默认是rbf
* C (string)：惩罚因子，默认为1.0

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="LogiPred">LogisticPredSPy3</a>
使用训练好的逻辑回归模型进行预测 (在原有预测数据上要先添加截距)

#### Tag:

* prediction

#### Param:

* None

#### Input:

* d_data1 (csv): 测试数据
* cols (py3pkl): 训练逻辑回归模型后筛选的变量
* m_fitted_model (py3pkl): 训练好的模型

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* d_feature (csv): 特征筛选后的自变量


## <a id="LogisticR">LogisticRegrSPy3</a>
logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 w‘x+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即y =w‘x+b，而logistic回归则通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b), 然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。

#### Tag:

* classifier

#### Param:

* penalty (string): 正则化（泛化）方法 (l1, l2)
* C (double): 正则化强度 (0-10)
* solve (string): 最优化方法 (liblinear, newton-cg, lbfgs, sag, saga)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="Logistic_WOE">LogisticRegr_WOE_ClasSPy3</a>
对WOE后的数据放入逻辑回归模型进行训练和特征筛选

#### Tag:

* classifier

#### Param:

* method (string)：特征筛选方法，逐步回归或者随机森林
* features_to_keep (int): 随机森林筛选法选择保留的变量数目

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量

#### Output:

* m_fitted_model (py3pkl): 训练好的模型
* o_summary (txt): 各变量训练后指标
* o_cols (py3pkl): 筛选后的变量
* o_feature_importances (pdf): 随机森林筛	选法特征重要性可视化


## <a id="MapLambda">MapLambdaDataSPy3</a>
对数据使用map函数从而进行特征的清洗

#### Tag:

* dataframe

#### Param:

* script (string): 填写map函数内部语句，例如：lambda x: int(x.replace("hey","")
* var (string): 选择清洗的变量

#### Input:

* d_data1 (py3pkl): 数据
 
#### Output:

* d_data2 (py3pkl): 处理后的数据



## <a id="Mapping">MappingDataSPy3</a>
类别字段映射

#### Tag:

* dataframe

#### Param:

* cols (string): 指定变量

#### Input:

* d_data1 (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 映射后数据


## <a id="MergeData">MergeDataSPy3</a>
根据主键对数据进行合并操作

#### Tag:

* dataframe

#### Param:

* key (string): 定义主键变量
* method (string): 选择合并方式

#### Input:

* d_data1 (csv): 数据1
* d_data2 (csv): 数据2

#### Output:

* d_changed_data (csv): 合并后数据


## <a id="MMSFitD">MinMaxScalerFitDataSPy3</a>
对连续型变量进行归一化处理, 使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素, 用于训练数据。

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 归一化后的数据
* m_minmax_model (py3pkl): 归一化训练好的模型


## <a id="MMSTransformD">MinMaxScalerTransformDataSPy3</a>
对连续型变量进行归一化处理, 使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素，将训练集中训练好的模型用于测试数据。

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据
* model (py3pkl): 归一化训练好的模型

#### Output:

* d_changed_data (py3pkl): 归一化后的数据


## <a id="MDropD">MissingDropDataSPy3</a>
删除几乎拥有唯一值的字段(比如单个变量最大类别占比大于95%)；删除缺失百分比大于一定比率的字段(比如类别变量大于30%，连续变量大于60%)。

### Tag:

* preprocessing

### Param:

* percent_obj (int): object型变量删除阈值
* percent_non_obj (int): 非object型变量删除阈值
* percent_unique (double): 唯一值变量删除阈值
 
### Input:

* d_data (py3pkl): 数据

### Output:

* d_changed_data (py3pkl)： 字段删除后的数据


## <a id="MFillD">MissingFillDataSPy3</a>
小比例缺失值用众数或中位数填充(例如，类别变量缺失小于10%时用众数填充，非类别变量缺失小于30%时用中位数填充)。

### Tag:

* preprocessing

### Param:

* percent_obj (int): 类别变量填充阈值
* percent_non_obj (int): 非类别变量填充阈值
 
### Input:

* d_data (py3pkl): 数据

### Output:

* d_changed_data (py3pkl): 缺失值填充后的数据


## <a id="MImputeD">MissingImputeDataSPy3</a>
用传播算法对缺失值进行填充

### Tag:

* preprocessing

### Param:

* lower_null_percent (int): 类别变量填充阈值下限
* upper_null_percent (int): 类别变量填充阈值上限 
* lower_null_percent1 (int): 非类别变量填充阈值下限
* upper_null_percent1 (int): 非类别变量填充阈值上限
 
#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 缺失值填充后的数据


## <a id="OneHot">OneHotEncoderDataSPy3</a>
将数据转化为虚拟数据，即将数据拆分为多个以0或1表示的列

#### Tag:

* preprocessing

#### Param:

* None


#### Input:

* d_data (py3pkl): 输入数据

#### Output:

* d_changed_data (py3pkl): 独热编码转换后数据


## <a id="PKL2CSV">PKL2CSVUnivSPy3</a>
将pkl转换为csv格式

#### Tag:

* utils

#### Param:

* None

#### Input:

* d_data1 (py3pkl): py3pkl数据
 
#### Output:

* d_data2 (csv): 返回csv格式数据


## <a id="PKLtoS">PKLtoSotrage</a>
将PKL文件导入个人存储空间中

#### Tag:

* utils

#### Param:

* output_name (sting): 填写输出文件的名字 

#### Input:

* o_pkl_file (py3pkl): 读入的PKL文件


#### Output:

* None


## <a id="PKLfromS">PKLfromStorage</a>
从个人存储空间中读入PKL文件

#### Tag:

* utils

#### Param:

* input_name (sting): 从个人存储空间读入文件名 

#### Input:

* None

#### Output:

* o_pkl_file (py3pkl): 输出PKL文件



## <a id="PLCurve">PlotLearningCurveSPy3</a>
画出学习曲线，根据训练集和测试集分数判断是否过拟合或欠拟合。

#### Tag:

* visualization

#### Param:

* n_jobs (int): 平行化运行工作的个数
* n_splits (int): 交叉验证时使用折数
* test_size (double): 验证集百分比
* model (string): 输入模型estimator, 例如GaussianNB()

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* learning_curve (jpg): 学习曲线图


## <a id="PLCBest">PlotLearningCurveSPy3_BestModel</a>
画出学习曲线，根据训练集和测试集分数判断是否过拟合或欠拟合。

#### Tag:

* visualization

#### Param:

* n_jobs (int): 平行化运行工作的个数
* n_splits (int): 交叉验证时使用折数
* test_size (double): 验证集百分比

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量
* best_model (py3pkl): 训练好的模型

#### Output:

* learning_curve (jpg): 学习曲线图


## <a id="PmmlC">PmmlClasSPy3</a>
将训练后的模型保存为PMML格式发布

#### Tag:

* model_release

#### Param:

* None

#### Input:

* m_fitted_model (py3pkl): 训练好的模型

#### Output:

* m_selected_fitted_model (pmml): 将模型以pmml格式输出


## <a id="Quantile">QuantileTrasformerDataSPy3</a>
分位数转换的目的是把特征数据转换到一定的范围内，或者让他们符合一定的分布。分位数转换利用的是数据的分位数信息进行变换。
它能够平滑那些异常分布，对于存在异常点的数据也很适合。但是它会破话原来数据的相关性和距离信息

#### Tag:

* preprocessing

#### Param:

* n_quantiles (int): 计算所用的分位数，默认值为1000
* output_distribution (string): 换数据遵循的分布函数，默认值为uniform
* ignore_implicit_zeros (string): 只针对稀疏矩阵，默认值为False，当为True时，则在计算分位数时稀疏矩阵中的稀疏条目将被忽略
* subsample (int): 用于计算有效分为数的最大样本数，默认值为100000
* copy (string): 是否复制原有数据

#### Input:

* d_data (py3pkl): 输入数据

#### Output:

* d_changed_data (py3pkl): 分位数转换后数据


## <a id="Rforest">RandomforestClasSPy3</a>
随机森林是利用多棵树对样本进行训练并预测的一种分类器，它是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

#### Tag:

* classifier

#### Param:

* n_estimators (int): 评估器数量
* criterion (string): 特征选择方法

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="RforestR">RandomforestRegrSPy3</a>
随机森林是利用多棵树对样本进行训练并预测的一种分类器，它是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

#### Tag:

* regressor

#### Param:

* n_estimators (int): 评估器数量
* criterion (string): 特征选择方法

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_importance_feat (csv): 特征重要性
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="RSearch">RandomizedSearchSPy3</a>
随机搜索交叉验证调参

#### Tag:

* hyperparameters_tuning

#### Param:

* n_iter_search (int): 运行随机搜索的次数
* model (string): 输入模型因子，例如RandomForestClassifier(n_estimators=20)
* param_dist (string): 填入需要随机搜索的参数
* whether_param_search (int): 选择是否需要进行随机搜索，如果值为0, 则不进行搜索，直接输出模型。

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* best_params (txt): 最优参数
* best_model (py3pkl): 最优模型(py3pkl格式)
* best_model_txt (txt): 最优模型(txt格式)



## <a id="REvalS">RegrEvalSPy3</a>
对回归模型进行评估（包括mae,mse,r2等）

#### Tag:

* metrics

#### Param:

* multioutput (string): 计分方法 (uniform_average, raw_values, variance_weighted)


#### Input:

* d_true (csv): 真实标签
* d_pred (csv): 预测标签

#### Output:

* o_metric (csv): 返回的模型各评估值


## <a id="ReplaceD">ReplaceDataSPy3</a>
将数据中的某个值全部替换为另一个值

#### Tag:

* dataframe

#### Param:

* value_before1 (string): float型要替换的值，默认参数为#时，选择value_before2作为要替换的值
* value_before2 (string): 字符串类型要替换的值
* value_after (string): 替换后的值

#### Input:

* d_data1 (py3pkl): 替换前的数据
 
#### Output:

* d_data2 (py3pkl): 替换后的数据


## <a id="RPDFCE">ReportPDFClasEvalSPy3</a>
输出分类评估报告

#### Tag:

* metrics
* visualization

#### Param:

* None

#### Input:

* o_metric (csv): 各评估指标（准确率、Kappa分数、F分数、ROC值等）
* o_classification_report (txt): 评估报告(F分数、精确率、召回率)
* o_confusion_matrix (jpg): 混淆矩阵图
* o_roc_curve (jpg): ROC曲线图
* o_metric_2 (csv): 各评估指标（准确率、Kappa分数、F分数、ROC值等）
* o_classification_report_2 (txt): 评估报告(F分数、精确率、召回率)
* o_confusion_matrix_2 (jpg): 混淆矩阵图
* o_roc_curve_2 (jpg): ROC曲线图

#### Output:

* evaluation_report (pdf): 评估报告


## <a id="RFEF">RFEFeatSPy3</a>
递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一边，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。

#### Tag:

* feature_selection

#### Param:

* percent_to_keep (double): 保留变量个数百分比 (0-1)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量

#### Output:

* d_changed_data (csv): 递归特征消除后的数据
* d_rfe_support (html): 统计哪些变量保留，哪些不保留
* rfe_cols (py3pkl): 筛选后保留的变量


## <a id="SpectralC">SpectralClusSPy3</a>
谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用。它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。

#### Tag:

* cluster

#### Param:

* n_clusters (int): 指定聚类个数
* affinity (string): 指定核函数

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量
 
#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型


## <a id="SplitF">SplitFeatSPy3</a>
将特征数据集和标签数据集拆分为训练集（特征、标签），测试集（特征、标签）。

#### Tag:

* dataframe

#### Param:

* test_size (double): 测试集比例 (0-1.0)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量

#### Output:

* d_feature_train (csv): 训练集特征变量
* d_feature_test (csv): 标签集特征变量
* d_label_train (csv): 训练集标签变量
* d_label_test (csv): 标签集特征变量


## <a id="SQL">SQLUnivSPy3</a>
可通过sql脚本编辑器编写sql语句

#### Tag:

* SQL

#### Param:

* Script (string): sql语句

#### Input:

* d_data1 (csv): 输入数据
 
#### Output:

* d_data2 (csv): 输出数据


## <a id="StackingC">StackingClasSPy3</a>
堆栈模型：分为两层，第一层是几个模型的集合，第二层是单独的一个模型，用第一层几个模型的输出作为第二层的输入来训练元模型。

#### Tag:

* classifier

#### Param:

* folds (int): 在拟合时使用的折数
* shuffle (string): 是否在分折之前洗牌数据
* random_state (int): 随机种子数
* verbose (int): 打印日志信息详细程度
* n_jobs (int): 在模型训练和预测过程中使用的计算机核数

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量
* m_base_learners (py3pkl): 基础模型(第一层)
* m_meta_learner (py3pkl): 元模型(第二层)
 
#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="StackingPredict">StackingPredictSPy3</a>
根据训练好的Stacking模型对数据进行预测

#### Tag:

* prediction

#### Param:

* None

#### Input:

* d_feature (csv): 特征变量
* m_fitted_model (py3pkl): 训练好的模型
 
#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* d_predict (CSV): 预测值和预测概率


## <a id="TPOT">TPOTSPy3</a>
基于Python的自动式机器学习工具，使用基因算法优化机器学习管道。

#### Tag:

* AutoML

#### Param:

* split_radio (double): 训练数据分割比
* generation (int): 对运行管道优化过程的迭代次数
* population_size (int): 每次迭代个体保留数目
* tpop_mode (string): 确定模型是监督分类或回归问题
* cv (int): 在评估管道时使用的交叉验证策略
* verbosity (int): 显示日志信息详尽等级
* scoring (string): 模型评估选择指标
* n_jobs (int): 在TPOT优化过程中，并行用于评估管道的过程的数量

#### Input:

* in_1 (csv): 输入数据

#### Output:

* out_1 (file): 输出自动生成的python代码


## <a id="VTFitF">VarianceThresholdFitFeatSPy3</a>
根据方差去掉取值变化小的特征，用于训练集数据。

#### Tag:

* feature_selection

#### Param:

* None

#### Input:

* d_feature (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 方差筛选后的数据
* model (py3pkl): 方差筛选训练好的模型


## <a id="VTTransformF">VarianceThresholdTransformFeatSPy3</a>
根据方差去掉取值变化小的特征，使用训练集训练好的模型于测试集数据。

#### Tag:

* feature_selection

#### Param:

* None

#### Input:

* d_feature (py3pkl): 数据
* model (py3pkl): 方差筛选训练好的模型

#### Output:

* d_changed_data (py3pkl): 方差筛选后的数据


## <a id="WOEIV">WOE_IV_DataSPy3</a>
IV的全称是Information Value，中文意思是信息价值，或者信息量。我们需要一些具体的量化指标来衡量每个自变量的预测能力，并根据这些量化指标的大小，来确定哪些变量进入模型。IV就是这样一种指标，他可以用来衡量自变量的预测能力。类似的指标还有信息增益、基尼系数等等。高IV表示该特征和目标变量的关联度高；目标变量只能是二分类；特征分箱越细，IV越高。

WOE的全称是“Weight of Evidence”，即证据权重。WOE是对原始自变量的一种编码形式。WOE表示的实际上是“当前分组中响应客户占所有响应客户的比例”和“当前分组中没有响应的客户占所有没有响应的客户的比例”的差异。

#### Tag:

* preprocessing
* visualization

#### Param:

* label (string)：定义标签变量

#### Input:

* d_data1 (py3pkl): 输入数据
* o_all_var (py3pkl): 所有需要计算WOE的变量

#### Output:

* o_IV_bar_plot (jpg): IV分布柱形图
* o_WOE_corr_plot (jpg): 计算WOE后的相关性热力图
* o_vars_after_corr (txt): 根据阈值相关性筛选后剩余的变量
* o_vars_after_VIF (txt): 多重共线性筛选后剩余的变量
* d_data2 (py3pkl): 计算WOE后的数据输出
* o_vars_after_VIF_pkl (py3pkl): 多重共线性筛选后剩余的变量 (pkl格式)


## <a id="word2cut">word2cut</a>
用jieba进行分词

#### Tag:

* natural_language_processing

#### Param:

* None 

#### Input:

* text_feature (py3pkl): 输入的文本

#### Output:

* cut_feature (py3pkl): 分词后文本(pickle格式)
* cutted (txt): 分词后文本(txt格式可查看)


## <a id="word2vec">word2vec</a>
把文本特征转成词向量(词计数矩阵)

#### Tag:

* natural_language_processing

#### Param:

* None 

#### Input:

* vectorizer (py3pkl): 训练后的词向量模型
* text_feature (py3pkl): 分词后文本

#### Output:

* vec_feature (py3pkl): 词向量


## <a id="wordfitting">wordfitting</a>
训练词向量模型(将文档转换为词计数矩阵)

#### Tag:

* natural_language_processing

#### Param:

* None 

#### Input:

* word_set (py3pkl): 分词后文本

#### Output:

* vectorizer (py3pkl): 训练后的词向量模型


## <a id="XGboostC">XGboostClasSPy3</a>
在过滤数据样例寻找分割值时，XGBoost 通过预分类算法和直方图算法来确定最优分割。XGBoost本身无法处理分类变量，而是像随机森林一样，只接受数值数据。因此在将分类数据传入 XGBoost 之前，必须通过各种编码方式：例如标记编码、均值编码或独热编码对数据进行处理。

#### Tag:

* classifier

#### Param:

* learning_rate (double): 学习速率
* n_estimators (int): 评估器数量
* max_depth (int): 最大树深度 
* min_child_weight (double): 最小叶子节点权重和
* gamma (double): 分叉时需要的最小损失减少
* subsample (double): 训练每棵树时用来训练的数据占全部的比例
* colsample_bytree (double): 训练每棵树时用来训练的特征的比例
* objective (string): 目标函数定义

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="zixun">zixun_word2vec_copy</a>
用gensim对原始的文本进行分词、去除停用词等操作，得到特征列表，再训练词向量模型得到词向量

#### Tag:

* natural_language_processing

#### Param:

* folderPath (string): 源文件路径
* file_encoding (string): 文件编码格式
* mincount (int): 忽略所有频数小于mincount的词

#### Input:

* stopwords (txt): 停用词
* user_dict (txt): 原始文本

#### Output:

* fileSegWordPath (txt): 分词后文本
* model: 训练的词向量模型


## <a id="ML_local">ML_localfile2csv</a>
自动建模 - 把数据模块转换为csv文件

#### Tag:

* AutoML

#### Param:

* None

#### Input:

* DS (any): 连接本地数据模块

#### Output:

* originalData (csv): 数据输出


## <a id="ML_prehandle">ML_PreHandle</a>
自动建模 - 缺失值处理 - 可以选择“不做处理”、“缺失值填补”、“删除行”三种方法; 针对类别型变量，可以选择“众数”、“常数”做填补；针对数值型变量，可以选择“众数”、“平均数”、”中位数“、”常数“做填补

#### Tag:

* AutoML

#### Param:

* cols (string): 列名，多列用逗号分割
* variableType (string): 列的类型
* handling (string): 缺失值处理策略 ASREGULAR - 作为常规值 IMPUTE - 填充 DROPROWS - 删除该行
* impute (string): 缺失值填充策略 MOSTFREQUENT-众数, MEAN-平均数 MEDIAN-中位数 CONSTANT-固定值
* constantValue (string): 缺失值填充固定值
* targetCol (string): 目标列

#### Input:

* originalData (csv): 输入的数据

#### Output:

* handleData (csv): 输出的数据


## <a id="ML_sample">ML_SampleData</a>
自动建模 - 数据采样

#### Tag:

* AutoML

#### Param:

* method (string): 采样方法	
* recordsNum (int): 采样行数	
* recordsRatio (double): 采样比例	
* column (string): 保持类平衡采样的特征列
* cols (string): 要采样的列，多列使用逗号分割

#### Input:

* handleData (csv): 输入的数据

#### Output:

* sampledata (csv): 采样后数据


## <a id="ML_category">ML_CategoryFeatureHandle</a>
自动建模 - 类别特征处理

#### Tag:

* AutoML

#### Param:

* cols (string): 类别列名，多列用逗号分割		
* handling (string): 特征处理策略		
* targetCol (string): 目标列，仅支持一列		

#### Input:

* sampledata (csv): 输入的数据

#### Output:

* categoryFeatureHandleData (csv): 类别特征处理后的数据


## <a id="ML_num">ML_NumFeatureHandle</a>
自动建模 - 数值特征处理

#### Tag:

* AutoML

#### Param:

* targetCol (string): 目标列			
* cols (string): 数值列，多列使用逗号分割		
* handling (string): 数值特征处理策略
* rescaling (string): handling为ASREGULARFEATURE必填	
* binarizeThreshold (string): handling为BINARIZETHRESHOLD必填		
* constantValue (double): 固定值，binarizeThreshold为CONSTANT必填		
* quantizeNum (int): handling为QUANTIZE必填			

#### Input:

* sampledata (csv): 输入的数据

#### Output:

* numFeatureHandleData (csv): 数值特征处理后的数据


## <a id="ML_split">Ml_SplitSet</a>
自动建模 - 数据拆分

#### Tag:

* AutoML

#### Param:

* targetCol (string): 目标列			
* split (string): 拆分策略	
* kFoldCrossTest (string): 是否启用K-折交叉
* foldNum (int): K-折，kFoldCrossTest=True	
* trainRatio (double): 训练集的样本比例，kFoldCrossTest=FALSE		

#### Input:

* featureHandleData (csv): 输入的数据

#### Output:

* X_train (csv): 训练集X
* y_train (csv): 训练集y
* X_test (csv): 测试集x
* y_test (csv): 测试集y


## <a id="ML_HyperparamsCV">ML_HyperparamsCV</a>
自动建模 - 超参数调优模块

#### Tag:

* AutoML

#### Param:

* hyperparams (string): 超参数配置				
* param_dist (string): 算法参数信息		
* evalStr (string): 算法名称	

#### Input:

* X_train (csv): 训练数据x
* y_train (csv): 训练数据y

#### Output:

* blockId (py3pkl): 模块ID
* model (model.pkl): 最优模型


## <a id="ML_MultiClassModelEval">ML_MultiClassModelEval</a>
自动建模 - 多分类评估模块

#### Tag:

* AutoML

#### Param:

* None

#### Input:

* blockId (py3pkl): 模块ID
* model (model.pkl): 最优模型
* X_test (csv): 测试数据x
* y_test (csv): 测试数据y

#### Output:

* modelScores (py3pkl): 模型评估分数


## <a id="ML_RegModelEval">ML_RegModelEval</a>
自动建模 - 回归模型评估

#### Tag:

* AutoML

#### Param:

* None

#### Input:

* blockId (py3pkl): 模块ID
* model (model.pkl): 最优模型
* X_test (csv): 测试数据x
* y_test (csv): 测试数据y

#### Output:

* modelScores (py3pkl): 模型评估分数


## <a id="ML_BinaryClass">ML_BinaryClassModelEval</a>
自动建模 - 二分类评估模块

#### Tag:

* AutoML

#### Param:

* None

#### Input:

* blockId (py3pkl): 模块ID
* model (model.pkl): 最优模型
* X_test (csv): 测试数据x
* y_test (csv): 测试数据y

#### Output:

* modelScores (py3pkl): 模型评估分数


## <a id="Pivot_pyspark">PivotingDPy3</a>
选择指定列进行数据透视操作，分布式数据探索

#### Tag:

* data_exploration_pyspark

#### Param:

* cols (string): 分组列				
* var1 (string): 透视列		
* var2 (string): 求和列	

#### Input:

* in1 (string): 输入

#### Output:

* out1 (any): 输出

## <a id="GroupbySum_pyspar">GroupbySumDPy3</a>
根据指定变量进行分组并求和、取均值、取最大值、取最小值、计数，分布式数据探索

#### Tag:

* data_exploration_pyspark

#### Param:

* cols (string): groupby变量				
* col (string): 指定做操作的变量		
* operation (string): 指定操作类型	

#### Input:

* in1 (string): 输入

#### Output:

* out1 (any): 输出

## <a id="Correlate_pyspark">CorrelationDPy3</a>
计算变量间相关系数矩阵，分布式数据探索

#### Tag:

* data_exploration_pyspark

#### Param:

* None				


#### Input:

* in1 (any): 输入

#### Output:

* None

## <a id="FeatInfo_pyspark">FeatureInfoDPy3</a>
数据信息统计：行数统计、变量数统计、缺失值百分比统计、各变量缺失值统计，分布式数据探索

#### Tag:

* data_exploration_pyspark

#### Param:

* None				

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

* out2 (any): 输出

* out3 (any): 输出

* out4 (any): 输出

* out5 (any): 输出

## <a id="ChiSquare_pyspark">ChiSquareTestDPy3</a>
卡方检验自变量与因变量之间关系，分布式数据探索

#### Tag:

* data_exploration_pyspark

#### Param:

* label (string): 目标变量				

#### Input:

* in1 (any): 输入

#### Output:

* None

## <a id="VecSlice_pyspark">VectorSlicerDPy3</a>
VectorSlicer是一个变换器，它采用一个特征向量并输出一个带有原始特征子阵列的新特征向量。 它对于从向量列中提取变量非常有用。分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* indices (string): 选择指定变量的index				

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="VecIdx_pyspark">VectorIndexerDPy3</a>
根据最大类别数识别类别变量，然后对向量中的类别变量索引化，主要作用是提高决策树或随机森林算法的分类效果。分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* maxCategories (int): 定义类别型变量的最大类别数				

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="VecAssemble_pyspark">VectorAssemblerDPy3</a>

将给定的多列表组合成一个单一的相量列。分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* label (string): 标签变量

* cols (string): 如果为#，则组合除标签外所有变量

* method (string): 	选择变量方法

#### Input:

* in1 (any): 输入

* in2 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="StrIdx_pyspark">StringIndexerDPy3</a>

将字符串列编码为标签索引列。分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* cols (string): 要转换的变量，如果#，则对输入的字符串变量进行转换

* label (string): 标签

#### Input:

* in1 (any): 输入

* in2 (any): 输入

* in3 (any): 输入

#### Output:

* out1 (any): 输出

* out2 (any): 输出

## <a id="StdScale_pyspark">StandardScalerDPy3</a>

StandardScaler转换数据集的向量行，将每个变量标准化为具有单位标准差和/或零均值。分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* withMean (string): 在缩放之前是否使用均值将数据居中。 它会建立一个密集的输出，所以在应用稀疏输入时要小心


#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="StackRow_pyspark">StackRowsDPy3</a>

对两个数据集进行行堆积，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* None

#### Input:

* in1 (any): 输入

* in2 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="QuantileDiscret_pyspark">QuantileDiscretizerDPy3</a>

对连续变量进行分箱，转为离散型变量，分箱个数可以自己设定，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* cols (string): 可以选择要处理的变量，如果#则根据输入的非字符串变量进行处理

* handleInvalid (string): 遇到缺失值的处理方法
#### Input:

* in1 (any): 输入

* in2 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="PolyExpansion_pyspark">PolynomialExpansionDPy3</a>

特征工程：多项式扩展变量，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* degree (int): 扩展至的维度

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="PCA_pyspark">PCADPy3</a>

PCA降维使用正交变换方法将一组互相相关的变量转换为一组线性非相关的变量，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* k (int): pca降至维数

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="OneHot_pyspark">OneHotEncoderDPy3</a>


对类别型变量进行独热编码，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* None

#### Input:

* in1 (any): 输入

* in2 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="Normlize_pyspark">NormalizerDPy3</a>


将每个向量归一化为单位范数，使用何种范数进行归一化可以自行设定，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* p (int): 选择采用哪种范数

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="MinMax_pyspark">MinMaxScalerDPy3</a>


MinMaxScaler转换数据集的向量行，将每个特征重新缩放到特定范围，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* None

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="MergeCols_pyspark">MergeColsDPy3</a>


通过共有列组合数据，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* method (string): 合并方式

#### Input:

* in1 (any): 输入

* in2 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="MaxAbsScl_pyspark">MaxAbsScalerDPy3</a>


MaxAbsScaler转换矢量行的数据集，通过除以每个变量的最大绝对值，将每个变量重新缩放到范围[-1,1]。 它不会中心化数据，因此不会破坏任何稀疏性，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* None

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="Idx2Str_pyspark">IndexToStringDPy3</a>


将经过数值编码后的变量转回原标签，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* cols (string): 要转换的变量

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="Impute_pyspark">ImputerDPy3</a>


Imputer用均值或中位数填补数据中缺失值，要填补的列必须是DoubleType或FloatType，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* strategy (string): 填补缺失值策略

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="FeatHash_pyspark">FeatureHasherDPy3</a>


FeatureHasher直接对特征应用一个hash函数来决定特征在样本矩阵中的列索引。这样的做法使得计算速度提升并且节省了内存，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* cols (string): 指定变量或选择出标签外所有变量

* label (string): 标签列

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="DropNan_pyspark">DropNADPy3</a>


根据条件删除缺失值，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* method (string): 删除缺失值方法

* thresh (string): 每行非缺失值个数不能小于该阈值，否则删除该行

* cols (string): 定义变量范围

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="ChiSelect_pyspark">ChiSqSelectorDPy3</a>


卡方检验筛选变量，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* label (string): 定义标签变量

* numTopFeatures (int): 卡方检验保留的变量个数

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="FillNan_pyspark">FillNADPy3</a>


用固定值填充缺失值，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* value (int): 将缺失值替换为该数值

* cols (string): 指定要填充的列

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="Split_pyspark">SplitDPy3</a>


分割训练集和测试集，分布式数据探索

#### Tag:

* preprocessing_pyspark

#### Param:

* testRate (double): 测试集比例

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

* out2 (any): 输出


## <a id="DTClas_pyspark">DecisionTreeClassifierDPy3</a>


分布式决策树分类模型

#### Tag:

* classifier_pyspark

#### Param:

* maxDepth (int): 最大深度

* maxBins (int): 每个特征分裂时最大划分数量

* minInstancesPerNode (int): 落在某一决策点上的最少实例数目

* minInfoGain (double): 最小信息增益

* impurity (string): 不纯度

* seed (double): 随机种子

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出


## <a id="MLPClas_pyspark">MultilayerPerceptronClassifierDPy2</a>


多层感知机分布式分类器

#### Tag:

* classifier_pyspark

#### Param:

* testRate (double): 训练集、测试集切分比例

* label (string): 数据集标签

* maxIter (int): 最大迭代次数

* tol (double): 收敛判据

* seed (double): 随机种子

* layers (string): 输入层到输出层的层数

* blockSize (int): 数据入栈的块大小

* stepSize (double): 学习步长

* solver (string): 优化算法

* initialWeights (string): 模型的初始权重

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出


## <a id="NB_pyspark">NaiveBayesDPy2</a>


朴素贝叶斯分布式分类器

#### Tag:

* classifier_pyspark

#### Param:

* testRate (double): 训练集、测试集切分比例

* label (string): 数据集标签

* smoothing (double): 平滑处理

* modelType (string): 模型方法Multinomial、Bernoulli

* thresholds (string): 预测分类时的决策阈值

* weightCol (string): 设定对应列所有实例权重为1.0

#### Input:

* in1 (any): 输入

#### Output:

* out1 (model.pmml): 输出

## <a id="RFClas_pyspark">RandomForestClassifierDPy2</a>


随机森林分布式分类器

#### Tag:

* classifier_pyspark

#### Param:

* testRate (double): 训练集、测试集切分比例

* label (string): 数据集标签

* maxDepth (int): 最大深度

* minInstancesPerNode (int): 落在某一决策点上的实例的最少个数

* minInfoGain (double): 最小信息增益

* numTrees (int): 树的个数

* seed (double): 随机种子

* maxBins (int): 每个特征分裂时最大划分数量

* impurity (string): 不纯度

#### Input:

* in1 (any): 输入

#### Output:

* out1 (model.pmml): 输出

## <a id="GBTClas_pyspark">GBTClassifierDPy2</a>


GBT分类pipeline: StringIndexer, VectorIndexer, LogisticRegression, IndexToString

#### Tag:

* classifier_pyspark

#### Param:

* testRate (double): 训练集、测试集切分比例

* label (string): 数据集标签

* maxDepth (int): 最大深度

* minInstancesPerNode (int): 落在某一决策点上的实例的最少个数

* minInfoGain (double): 最小信息增益

* seed (double): 随机种子

* lossType (string): 损失函数

* stepSize (double): 学习步长

* maxIter (int): 最大迭代次数

#### Input:

* in1 (any): 输入

#### Output:

* out1 (model.pmml): 输出

## <a id="LR3_pyspark">LogisticRegressionDPy3</a>


逻辑回归pipeline: StringIndexer, VectorIndexer, LogisticRegression, IndexToString

#### Tag:

* classifier_pyspark

#### Param:

* label (string): 数据集标签

* maxIter (int): 最大迭代次数

* regParam (double): 正则化参数

* elasticNetParam (double): 弹性网络混合参数，范围[0,1]

* threshold (double): 预测分类时的决策阈值

* testRate (double): 训练集、测试集切分比例

#### Input:

* in1 (any): 输入

#### Output:

* out1 (model.pmml): 输出

## <a id="LR2_pyspark">LogisticRegressionDPy2</a>


逻辑回归pipeline: StringIndexer, VectorIndexer, LogisticRegression, IndexToString

#### Tag:

* classifier_pyspark

#### Param:

* label (string): 数据集标签

* maxIter (int): 最大迭代次数

* regParam (double): 正则化参数

* elasticNetParam (double): 弹性网络混合参数，范围[0,1]

* threshold (double): 预测分类时的决策阈值

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出


## <a id="DTReg_pyspark">DecisionTreeRegressorDPy3</a>


分布式决策树回归模型

#### Tag:

* regressor_pyspark

#### Param:

* testRate (double): 训练集、测试集切分比例

* label (string): 数据集标签

* maxDepth (int): 最大深度

* maxBins (int): 每个特征分裂时最大划分数量

* minInstancesPerNode (int): 落在某一决策点上的实例的最少个数

* minInfoGain (double): 最小信息增益

* impurity (string): 不纯度

* seed (double): 随机种子

* varianceCol (string): 预测中偏置样本方差的列名


#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出


## <a id="RFReg_pyspark">RandomForestRegressorDPy3</a>


分布式随机森林回归模型

#### Tag:

* regressor_pyspark

#### Param:

* maxDepth (int): 最大深度

* maxBins (int): 每个特征分裂时最大划分数量

* minInstancesPerNode (int): 落在某一决策点上的实例的最少个数

* minInfoGain (double): 最小信息增益

* impurity (string): 不纯度

* seed (double): 随机种子

* numTrees (int): 树的个数

* featureSubsetStrategy (string): 切分每个结点时考虑的特征数目

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="LR_pyspark"LinearRegressionDPy3</a>


分布式线性回归模型

#### Tag:

* regressor_pyspark

#### Param:


* maxIter (int): 最大迭代次数

* regParam (double): 正则化参数

* elasticNetParam (double): 回归混合参数，等于0时为l2惩罚，等于1时为l1惩罚

* tol (double): 收敛判据

* fitIntercept (string): 是否考虑截距

* standardization (string): 是否标准化

* solver (string): 优化算法

* weightCol (string): 将该列所有实例权重设置为1.0

* aggregationDepth (int): 聚集树时的建议深度

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="GLR_pyspark">GeneralizedLinearRegressionDPy2</a>


分布式广义线性回归模型

#### Tag:

* regressor_pyspark

#### Param:

* family (string): 直属族

* link (string): 关联函数

* fitIntercept (string): 是否考虑截距

* maxIter (int): 最大迭代次数

* tol (double): 收敛判据

* regParam (double): 正则化参数

* weightCol (string): 将该列所有实例权重设置为1.0

* solver (string): 优化算法

* linkPredictionCol (string): 链路预测列名

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="GBTReg_pyspark">GBTRegressorDPy2</a>


分布式提升回归树

#### Tag:

* regressor_pyspark

#### Param:

* testRate (double): 训练集、测试集切分比例

* label (string): 数据集标签

* maxDepth (int): 最大深度

* maxBins (int): 每个特征分裂时最大划分数

* minInstancesPerNode (int): 落在某一决策点上的实例的最少个数

* minInfoGain (double): 最小信息增益

* subsamplingRate (double): 用于学习每棵树的训练集比例

* lossType (string): 损失函数

* maxIter (int): 最大迭代次数

* stepSize (double): 学习步长

* seed (double): 随机种子

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出


## <a id="BisectingKMeans_pyspark">BisectingKMeansDPy3</a>

分布式BisectingKMeans聚类

#### Tag:

* cluster_pyspark

#### Param:

* featuresCol (string): 进行聚类的特征字段名

* predictionCol (string): 预测结果的字段名

* k (int): 设置你想聚成的类数

* maxIter (int): 最大迭代次数

* seed (string): 随机种子

* minDivisibleClusterSize (double): 聚类的类别最少元素个数

#### Input:

* input_path (any): 输入

#### Output:

* output_path (model.pmml): 输出


## <a id="GM_pyspark">GaussianMixtureDPy3_copy</a>

分布式GaussianMixture聚类

#### Tag:

* cluster_pyspark

#### Param:

* featuresCol (string): 进行聚类的特征字段名

* predictionCol (string): 预测结果的字段名

* k (int): 设置你想聚成的类数

* tol (double): 收敛判据

* maxIter (int): 最大迭代次数

* seed (string): 随机种子

* probabilityCol (string): 输出预测概率字段

#### Input:

* input_path (any): 输入

#### Output:

* output_path (model.pmml): 输出

## <a id="LDA_pyspark">LDADPy3_copy</a>

分布式LDA聚类

#### Tag:

* cluster_pyspark

#### Param:

* featuresCol (string): 进行聚类的特征字段名

* k (int): 设置你想聚成的类数

* maxIter (int): 最大迭代次数

* seed (string): 随机种子

* checkpointInterval (int): 设置检查点间隔（>=1），或不设置检查点（-1）

* optimizer (string): 估计LDA模型时使用的优化器

* learningOffset (double): 控制"online"算法的学习率

* learningDecay (double): 控制"online"算法，用来减小前面训练样本批次对最终模型的影响

* subsamplingRate (double): 控制"online"算法的采样率

* optimizeDocConcentration (string): 如果设置为true，每次 minibatch 之后计算参数 docConcentration的最大似然估计  

* docConcentration (string): 按逗号分隔

* topicConcentration (int):  主题关于文字的先验分布集中参数

* topicDistributionCol (string): 指标聚类话题的输出字段

* keepLastCheckpoint (string): 如果使用Checkpointing，则保存最新的Checkpoint，否则将删除Checkpoint

#### Input:

* input_path (any): 输入

#### Output:

* output_path (model.pmml): 输出

## <a id="Kmeans_pyspark">KMeansDPy3</a>

分布式Kmeans聚类

#### Tag:

* cluster_pyspark

#### Param:

* featuresCol (string): 进行聚类的特征字段名

* predictionCol (string): 预测结果的字段名

* k (int): 设置你想聚成的类数

* initMode (string): 设置kmeans模型类型

* initSteps (int): 初始步数

* tol (double): 收敛判据

* maxIter (int): 最大迭代次数

* seed (string): 随机种子

#### Input:

* input_path (any): 输入

#### Output:

* output_path (model.pmml): 输出

## <a id="ParamSearchTV_pyspark">ParamSearchTVSPy3</a>

通过留出法搜索参数

#### Tag:

* model_selection & hyperparameters_tuning_pyspark

#### Param:

* CV (int): 交叉验证K值

* Params (string): 参数字典

* Evaluator (string): 评估器

#### Input:

* Estimator (any): 输入

* DataFrame (any): 输入

#### Output:

* out1 (any): 输出

## <a id="ParamSearchCV_pyspark">ParamSearchCVDPy3</a>

通过交叉验证搜索参数

#### Tag:

* model_selection & hyperparameters_tuning_pyspark

#### Param:

* CV (int): 交叉验证K值

* Params (string): 参数字典

* Evaluator (string): 评估器

#### Input:

* Estimator (any): 输入

* DataFrame (any): 输入

#### Output:

* out1 (any): 输出

## <a id="ModelPred_pyspark">ModelPredictDPy3</a>

模型预测

#### Tag:

* prediction & metrics_pyspark

#### Param:

* None

#### Input:

* in1 (any): 输入

* in2 (model.pkl): 输入

#### Output:

* out1 (any): 输出

## <a id="ModelEval_pyspark">ModelEvaluateDPy3</a>

模型评估并将评估分数发送到MLServer

#### Tag:

* prediction & metrics_pyspark

#### Param:

* label (string): 定义标签变量

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出


## <a id="PipelineFit_pyspark">PipelineFitDPy3</a>

读入管道并根据数据训练出管道模型

#### Tag:

* model_release_pyspark

#### Param:

* None

#### Input:

* in1 (json): 输入

* in2 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="spark2pmml_pyspark">spark2pmmlDPy</a>

模型转换为pmml格式

#### Tag:

* model_release_pyspark

#### Param:

* None

#### Input:

* in1 (any): 输入

* in2 (model.pkl): 输入

#### Output:

* None

## <a id="RegexToken_pyspark">RegexTokenizerDPy3</a>

自定义分隔符分词，支持正则表达式

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* toLowercase (string): 是否将大写转成小写

* minTokenLength (int): 最小分割长度

* gaps (string): 是否设置空格

* pattern (string): 分隔符，支持正则表达式

#### Input:

* input_path (any): 输入

#### Output:

* output_path (any): 输出


## <a id="Token_pyspark">TokenizerDPy3</a>

空格分词

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="NGram_pyspark">NGramDPy3</a>

NGram

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* n (int): ngram的n值

#### Input:

* input_path (any): 输入

#### Output:

* output_path (any): 输出

## <a id="StopWordsRemove_pyspark">StopWordsRemoverDPy3</a>

去停用词

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* stopWords (string): 停用词逗号隔开

* caseSensitive (string): 布尔变量

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="IDF_pyspark">IDFDPy3_copy</a>

利用IDFModel获取特征向量（通常由HashingTF或CountVectorizer创建）并缩放每列

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* minDocFreq (int): 最低文档词频

#### Input:

* input_path (any): 输入

#### Output:

* output_path (any): 输出

## <a id="HashTF_pyspark">HashingTFDPy3</a>

将原始特征通过应用哈希函数映射到索引中

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* numFeatures (int): 功能维度

* binary (string): 是否二进制编码

#### Input:

* input_path (any): 输入

#### Output:

* output_path (any): 输出

## <a id="Word2Vec_pyspark">Word2VecDPy3</a>

将每个单词映射到一个唯一的固定大小向量

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* vectorSize (int): 向量维度大小

* minCount (int): 词最小总量

* numPartitions (int): 设置分区数

* stepSize (double): 步长

* maxIter (int): 最大迭代次数

* seed (int): 种子数

* windowSize (int): 窗口大小

* maxSentenceLength (int): 最大句子长度

#### Input:

* in1 (any): 输入

#### Output:

* out1 (any): 输出

## <a id="CountVec_pyspark">CountVectorDPy3</a>

将词汇生成文档的稀疏表示

#### Tag:

* natural_language_processing_pyspark

#### Param:

* inputCol (string): 输入字段名

* outputCol (string): 输出字段名

* minDF (double): 词汇表中的文档的最小数量

* minTF (double): 最小tf值

* vocabSize (int): 词大小

* binary (string): 是否二进制编码

#### Input:

* input_path (any): 输入

#### Output:

* output_path (any): 输出


## <a id="keras_test">keras_test</a>

keras测试

#### Tag:

* deep_learning

#### Param:

* None

#### Input:

* data (csv): 输入

#### Output:

* image (pdf): 输出


## <a id="tensorflow_test">tensorflow_test</a>

基于tensorflow框架，构建、训练神经网络模型，并对模型进行预测

#### Tag:

* deep_learning

#### Param:

* data_path (string): 数据存放地址

#### Input:

* None

#### Output:

* None

## <a id="pytorch_test">pytorch_test</a>

基于pytorch框架，构建2层神经网络，实现前向传播和反向传播算法

#### Tag:

* deep_learning

#### Param:

* None

#### Input:

* data (csv): 输入

#### Output:

* result (txt): 输出

## <a id="mxnet_test">mxnet_test</a>

基于mxnet框架，构建2层神经网络，实现前向传播和反向传播算法

#### Tag:

* deep_learning

#### Param:

* lr (double): 学习率

* num_epochs (int): 迭代次数

* batch_size (int): 梯度下降值

#### Input:

* data (csv): 输入

#### Output:

* out_1 (file): 输出

## <a id="imageclassifier_mxnet">imageclassifier_mxnet</a>


基于mxnet框架的图像分类demo

#### Tag:

* deep_learning

#### Param:

* model_name (string): 模型名称

#### Input:

* image (jpg): 输入

#### Output:

* result (txt): 输出

## <a id="DataLoadMnistKeras">DataLoadMnistKeras</a>

从指定路径加载划分为训练集和测试集的mnist数据，返回该路径

#### Tag:

* deep_learning

#### Param:

* path (string): 加载路径

#### Input:

* None

#### Output:

* mnist_path (txt): 输出

## <a id="DataPretreatmentMnistKeras">DataPretreatmentMnistKeras</a>

对数据集进行预处理，返回划分好的数据集和shape

#### Tag:

* deep_learning

#### Param:

* img_rows (int): 图像像素高度

* img_cols (int): 图像像素宽度

* num_classes (int): 分类数目

* train_num (int): 训练集数目

* test_num (int): 测试集数目

#### Input:

* path (txt): 加载路径

#### Output:

* x_train (csv): 训练集特征

* y_train (csv): 训练集标签

* x_train_shape (csv): 训练集特征的维度

* y_train_shape (csv): 训练集标签的维度

* x_test (csv): 测试集特征

* y_test (csv): 测试集标签

* x_test_shape (csv): 测试集特征的维度

* y_test_shape (csv): 测试集标签的维度

## <a id="ModelMnistKeras">ModelMnistKeras</a>

利用keras搭建一个两层的卷积神经网络模型

#### Tag:

* deep_learning

#### Param:

* img_rows (int): 图像像素高度

* img_cols (int): 图像像素宽度

* num_classes (int): 分类数目

#### Input:

* None

#### Output:

* model (h5): 输出

* model_json (json): 输出

## <a id="ModelStructShowKeras">ModelStructShowKeras</a>

利用model.summary打印模型概况

#### Tag:

* deep_learning

#### Param:

* None

#### Input:

* model (h5): 输入

#### Output:

* model_struct (png): 输出

## <a id="ModelTrainKeras">ModelTrainKeras</a>

进行模型训练，返回训练好的模型

#### Tag:

* deep_learning

#### Param:

* COMPILE_optimizer (string): 编译优化器

* COMPILE_loss (string): 编译损失函数

* COMPILE_metrics (string): 模型训练/测试的度量指标

* COMPILE_sample_weight_mode (string): 样本权重类型

* COMPILE_weighted_metrics (string): metrics权重列表

* COMPILE_target_tensors (string): 自定义目标张量

* FIT_epochs (int): 训练轮数目

* FIT_verbose (int): 训练过程日志显示

* FIT_batch_size (int): 训练样本数量/批次

* FIT_callbacks (string): 训练过程中回调函数集

* FIT_validation_split (double): 从训练集抽取验证集比例

* FIT_validation_data (string): 指定的验证集

* FIT_shuffle (int): 是否在每训练轮次打乱样本顺序

* FIT_class_weight (string): 训练样本类权重

* FIT_sample_weight (string): 样本权重(依赖样本权重类型为temporal)

* FIT_initial_epoch (int): 训练起始轮次（用于继续中断的训练）

* FIT_steps_per_epoch (string): 每轮次训练包含的步数/批次

* FIT_validation_steps (string): 处理验证集用的步数/批次（依赖训练步数/训练轮次）

* COMPILE_loss_weights (string): 损失函数权重列表

#### Input:

* model (h5): 输入

* x_train (csv): 训练集特征

* y_train (csv): 训练集标签

* x_train_shape (csv): 训练集特征的维度

* y_train_shape (csv): 训练集标签的维度


#### Output:

* model_trained (h5): 训练好的模型

* model_json (json): 模型的json格式

* blockId (pkl): 输出

## <a id="ModelEvaluateKeras">ModelEvaluateKeras</a>

用测试集数据进行模型评估

#### Tag:

* deep_learning

#### Param:

* mlServerAddr (string): mlserver地址

* modelName (string): 模型名称

* EVALUATE_batch_size (string): 批大小

* EVALUATE_verbose (string): 日志显示

* EVALUATE_sample_weight (string): 权值的numpy array

#### Input:

* model_trained (h5): 训练好的模型

* x_test (csv): 测试集特征

* y_test (csv): 测试集标签

* x_test_shape (csv): 测试集特征的维度

* y_test_shape (csv): 测试集标签的维度

* blockId (pkl): 输入

#### Output:

* None

## <a id="vis_R">NormalVisuSR_all</a>

报表展示可视化模块-静态图片

#### Tag:

* R

#### Param:

* x_data_factor_flag (string): 是否转化成维度因子,输入"y"/"Y"执行

* reorder_flag (string): 是否按y轴排序,"order"升序，"desc"降序,其他不排序

* lab_title (string): 图表标题内容

* x_lab_title (string): x轴标题内容

* y_lab_title (string): y轴标题内容

#### Input:

* df (csv): 输入

#### Output:

* pp1 (pdf): 输出

## <a id="load_R">NormalVisuSR_read_data</a>

数据处理模块-1

#### Tag:

* R

#### Param:

* model_data_test_val (string): 数据选择


#### Input:

* df (csv): 输入

#### Output:

* df_select (csv): 输出

## <a id="load_R">NormalVisuSR_read_data</a>

动态-饼图

#### Tag:

* R

#### Param:

* lab_title (string): 报表标题

* tooltip_text_val_up (string): 提示标签前缀

* tooltip_text_val_down (string): 提示标签后缀


#### Input:

* df (csv): 输入

#### Output:

* ppg (html): 输出


